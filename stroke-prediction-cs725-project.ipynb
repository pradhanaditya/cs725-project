{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/python3\n\n'''\nFirst objective is to analyse the data to identify main features, characteristics \nand patterns. Visualization is an effective way to gain some insights about the \ndata. \n\nWe have to be careful regarding the following: \n1. There might be some missing values and outliers in the data.\n2. There are categorical columns in the dataset\n\nIt is better to normalize the data as well.\n'''\n\nimport pandas\nimport numpy\nimport matplotlib.pyplot as plt\nimport seaborn\nimport missingno\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\nfrom sklearn.metrics import precision_recall_curve, plot_precision_recall_curve, average_precision_score\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\n\nfrom sklearn.utils import shuffle\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\nfrom tensorflow_addons import losses\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.metrics import classification_report\n\n# Create the dataframe of the csv file\ndf = pandas.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\n\n# Display summary of the dataframe\nprint(df.info())\n\n# By manual inspection, we can confirm that the 'id' column can be removed as it does not contain any\n# relevant information\ndf.drop('id', axis = 1, inplace = True)\n\ndf['gender'].value_counts()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-21T17:13:21.886260Z","iopub.execute_input":"2021-11-21T17:13:21.886931Z","iopub.status.idle":"2021-11-21T17:13:30.051876Z","shell.execute_reply.started":"2021-11-21T17:13:21.886873Z","shell.execute_reply":"2021-11-21T17:13:30.051059Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Check the 'age' attribute values\ndf.age.sort_values()\ntemp_df_age = df[df['stroke'] == 1]['age']\n\n# Check the least values of 'age' attribute\ntemp_df_age.nsmallest(6)\n\n# The age with value '1.32' seems invalid, it will be better to drop the instances with very small values of age\ndf = df[df['age'] > 13]","metadata":{"execution":{"iopub.status.busy":"2021-11-21T17:13:30.053841Z","iopub.execute_input":"2021-11-21T17:13:30.054491Z","iopub.status.idle":"2021-11-21T17:13:30.070317Z","shell.execute_reply.started":"2021-11-21T17:13:30.054447Z","shell.execute_reply":"2021-11-21T17:13:30.068908Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Check the correlation among features\nseaborn.heatmap(df.corr(), annot = True)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T17:13:30.071623Z","iopub.execute_input":"2021-11-21T17:13:30.072004Z","iopub.status.idle":"2021-11-21T17:13:30.597569Z","shell.execute_reply.started":"2021-11-21T17:13:30.071975Z","shell.execute_reply":"2021-11-21T17:13:30.596693Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Since there is only 1 row with the gender as 'Other', we can drop it as it won't be much relevant in the analysis and prediction\ndf = df.drop(df.loc[df['gender'] == 'Other'].index)\n\n# Checks which columns in the dataset have atleast one null value\ndef find_columns_with_null_value(dataframe):\n    cols_list = dataframe.columns\n    n_rows = len(dataframe)\n    result = []\n    for i in range(len(cols_list)):\n        if dataframe.count()[i] < n_rows:\n            result.append(cols_list[i])\n            print('Number of missing values in ' + str(cols_list[i]) + ' = ' + str(n_rows - dataframe.count()[i]))\n    return result\n\n'''\nFills the missing values in a column by the median value\nReference: https://stackoverflow.com/questions/18689823/pandas-dataframe-replace-nan-values-with-average-of-columns\n'''\ndef fill_missing_values_by_median(dataframe, column_name):\n    dataframe[column].fillna(dataframe[column].median(), inplace = True)\n    return dataframe\n\n# Checking relation between gender and stroke\nseaborn.countplot(x='gender', data = df, hue = 'stroke', palette=['yellow',\"red\"])\n\n# Number of rows in the dataframe\nprint('Number of rows in the dataframe: ' + str(len(df)))\nprint('Column(s) with atleast one NULL value (missing value): ' + str(find_columns_with_null_value(df)))","metadata":{"execution":{"iopub.status.busy":"2021-11-21T17:13:30.599491Z","iopub.execute_input":"2021-11-21T17:13:30.599747Z","iopub.status.idle":"2021-11-21T17:13:30.889513Z","shell.execute_reply.started":"2021-11-21T17:13:30.599718Z","shell.execute_reply":"2021-11-21T17:13:30.888471Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"''' \nThis function assumes that there does not exist any column in the dataset which has numerical\nvalues and which is categorical\n\nReference: https://stackoverflow.com/questions/29803093/check-which-columns-in-dataframe-are-categorical\n'''\ndef identify_categorical_columns(dataframe):\n    cols_list = dataframe.columns\n    numerical_cols_list = dataframe.select_dtypes('number').columns\n    result = list(filter(lambda column: (column not in numerical_cols_list), set(cols_list)))\n    return result\n\n# Find out the columns which have categorical values\nprint('Categorical Columns: ' + str(identify_categorical_columns(df)))","metadata":{"execution":{"iopub.status.busy":"2021-11-21T17:13:30.890839Z","iopub.execute_input":"2021-11-21T17:13:30.891061Z","iopub.status.idle":"2021-11-21T17:13:30.900943Z","shell.execute_reply.started":"2021-11-21T17:13:30.891035Z","shell.execute_reply":"2021-11-21T17:13:30.899928Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#Randomly shuffling the dataframe\n# df = shuffle(df, random_state = 42)\n\n'''\nThe attributes 'work_type' and 'smoking_status' have many possible values, so it will be better to encode these columns using one-hot encoding.\n\n'gender', 'ever_married', and 'Residence_type' attributes can be transformed into binary features using the LabelEncoder.\n\nReferences:\n1. https://towardsdatascience.com/handling-categorical-data-the-right-way-9d1279956fc6\n2. https://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python\n3. https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n'''\n\n# Encoding the 'work_type' and 'smoking_status' attributes using one-hot encoding\ntemp_columns = pandas.get_dummies(df.work_type)\ndf.drop('work_type', axis = 1, inplace = True)\ntemp_df1 = df[[column for column in df.columns[:5]]]\ntemp_df1 = pandas.concat([temp_df1, temp_columns], axis = 1)\ntemp_df2 = pandas.get_dummies(df.smoking_status)\ntemp_df3 = df[[column for column in df.columns[5:8]]]\ntemp_df1 = pandas.concat([temp_df1, temp_df3, temp_df2, df['stroke']], axis = 1)\n\n# Encoding the 'gender', 'ever_married' and 'Residence_type' attributes using label encoding\nlabel_encoder = LabelEncoder()\n\ntemp_df1['gender'] = label_encoder.fit_transform(temp_df1['gender'])\ntemp_df1['ever_married'] = label_encoder.fit_transform(temp_df1['ever_married'])\ntemp_df1['Residence_type'] = label_encoder.fit_transform(temp_df1['Residence_type'])\n\ndf = temp_df1\n\ndf","metadata":{"execution":{"iopub.status.busy":"2021-11-21T17:13:30.902408Z","iopub.execute_input":"2021-11-21T17:13:30.902961Z","iopub.status.idle":"2021-11-21T17:13:30.953858Z","shell.execute_reply.started":"2021-11-21T17:13:30.902917Z","shell.execute_reply":"2021-11-21T17:13:30.952793Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"'''\nCheck whether the dataset is balanced or imbalanced in terms of output labels.\n'''\n\ndata_balance_check_labels = ['stroke = 0', 'stroke = 1']\ntotal_instances_per_value = df['stroke'].value_counts()\npie_chart_colors = ['orange', 'red']\nplt.figure(figsize=(6,6))\nplt.pie(total_instances_per_value, labels = data_balance_check_labels, shadow = 1, explode = (0.1, 0), autopct='%1.2f%%', colors = pie_chart_colors)\nplt.title('Percentage of rows in the dataset where stroke = 0 and stroke = 1')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T17:13:30.955201Z","iopub.execute_input":"2021-11-21T17:13:30.955454Z","iopub.status.idle":"2021-11-21T17:13:31.107864Z","shell.execute_reply.started":"2021-11-21T17:13:30.955423Z","shell.execute_reply":"2021-11-21T17:13:31.107026Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"'''\nThis is the main problem in the dataset - the dataset is imbalanced. This might lead to overfit and the resulting model\nwill be biased towards predicting 'no stroke (stroke = 0)\n'''\n# Splitting the dataset into the train data (80%) and the test data (20%)\n# Reference: https://stackoverflow.com/questions/42191717/scikit-learn-random-state-in-splitting-dataset\n\ntrain_data, test_data = train_test_split(df, test_size = 0.2, random_state = 42)\n\n# Separating the input features and the output label from the training data and the test data\nX_train_data = train_data.copy()\nX_train_data = X_train_data.drop('stroke', axis = 1)\n# X_train_data = train_data[[column for column in train_data.columns[:-1]]]\n#y_train_data = train_data[[train_data.columns[len(train_data.columns) - 1]]]\ny_train_data = train_data[['stroke']]\n#X_test_data = test_data[[column for column in test_data.columns[:-1]]]\n#y_test_data = test_data[[test_data.columns[len(test_data.columns) - 1]]]\nX_test_data = test_data.copy()\nX_test_data = X_test_data.drop('stroke', axis = 1)\ny_test_data = test_data[['stroke']]\n'''\nIf there are any missing values in any column, fill them by the median value of that column.\nIf we do this before the train-test split, there is a possibility of data leakage.\nReferences: \n1. https://www.analyticsvidhya.com/blog/2021/05/dealing-with-missing-values-in-python-a-complete-guide/\n2. https://www.analyticsvidhya.com/blog/2021/07/data-leakage-and-its-effect-on-the-performance-of-an-ml-model/\n'''\nfor column in find_columns_with_null_value(X_train_data):\n    X_train_data = fill_missing_values_by_median(X_train_data, column)\n\nfor column in find_columns_with_null_value(X_test_data):\n    X_test_data = fill_missing_values_by_median(X_test_data, column)\n    \nprint(sum(y_train_data.values.ravel()))\nprint(sum(y_test_data.values.ravel()))\n","metadata":{"execution":{"iopub.status.busy":"2021-11-21T17:13:31.109213Z","iopub.execute_input":"2021-11-21T17:13:31.109462Z","iopub.status.idle":"2021-11-21T17:13:31.186584Z","shell.execute_reply.started":"2021-11-21T17:13:31.109430Z","shell.execute_reply":"2021-11-21T17:13:31.185528Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Scale the train and test data\ndata_scaler = MinMaxScaler()\nX_train_data_scaled = data_scaler.fit_transform(X_train_data)\nX_test_data_scaled = data_scaler.transform(X_test_data)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T17:13:31.188101Z","iopub.execute_input":"2021-11-21T17:13:31.188719Z","iopub.status.idle":"2021-11-21T17:13:31.201902Z","shell.execute_reply.started":"2021-11-21T17:13:31.188669Z","shell.execute_reply":"2021-11-21T17:13:31.201130Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Oversample the training data using Synthetic Minority Oversampling Technique (SMOTE)\n# Reference: https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\nsmote_for_oversampling = SMOTE(random_state = 42)\nX_train_oversampled, y_train_oversampled = smote_for_oversampling.fit_resample(X_train_data_scaled, y_train_data)\n#X_test_data, y_test_data = smote_for_oversampling.fit_resample(X_test_data, y_test_data)\nprint(y_train_oversampled.value_counts())\n\n# Random under-sampling removes the rows having majority class labels randomly\nrandom_under_sampler = RandomUnderSampler(replacement = True, random_state = 42)\n\nX_train_undersampled, y_train_undersampled = random_under_sampler.fit_resample(X_train_data_scaled, y_train_data)\n\nprint(y_train_undersampled.value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-11-21T17:13:31.204479Z","iopub.execute_input":"2021-11-21T17:13:31.205517Z","iopub.status.idle":"2021-11-21T17:13:31.233254Z","shell.execute_reply.started":"2021-11-21T17:13:31.205468Z","shell.execute_reply":"2021-11-21T17:13:31.232209Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"'''\n*** TODO ***\n1. More visualizations of data\n2. Normalization of data (if useful)\n3. Implementation of models and their analysis on both the non-sampled and the oversampled training data\n************\n\nReferences:\n1. https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n2. https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976\n3. https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\n4. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_numpy.html\n'''\n\n# Training and testing a logistic regression model on the oversampled training data\n# logistic_reg_pipeline =  make_pipeline(StandardScaler(), LogisticRegression(random_state = 42))\n# logistic_reg_pipeline.fit(X_train_oversampled, y_train_oversampled.values.ravel())\nlogistic_reg = LogisticRegression(random_state = 42)\nlogistic_reg.fit(X_train_oversampled, y_train_oversampled.values.ravel())\n# y_predicted_lr = logistic_reg_pipeline.predict(X_test_data_scaled)\ny_predicted_lr = logistic_reg.predict(X_test_data_scaled)\n\nconfusion_matrix_lr = confusion_matrix(y_test_data, y_predicted_lr)\nprint('Confusion Matrix: ')\nprint(confusion_matrix_lr)\n\nprint('Accuracy (in %): ' + str(logistic_reg.score(X_test_data_scaled, y_test_data)*100))\n\nprecision_lr = precision_score(y_test_data, y_predicted_lr)\nprint('Precision: ' + str(precision_lr))\n\nrecall_lr = recall_score(y_test_data, y_predicted_lr)\nprint('Recall: ' + str(recall_lr))\n\nf1_lr = f1_score(y_test_data, y_predicted_lr)\nprint('F1 score: ' + str(f1_lr))\n\n# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html\nplt.figure(figsize = (10, 7))\nseaborn.heatmap(confusion_matrix_lr, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 3, cbar = True, annot_kws = {'fontsize': 17},\n                xticklabels = ['No stroke (predicted)', 'Stroke (predicted)'], yticklabels = ['No stroke', 'Stroke'])\nplt.yticks(rotation = 0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T17:13:32.606361Z","iopub.execute_input":"2021-11-21T17:13:32.606875Z","iopub.status.idle":"2021-11-21T17:13:33.077858Z","shell.execute_reply.started":"2021-11-21T17:13:32.606840Z","shell.execute_reply":"2021-11-21T17:13:33.076855Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Training and testing a SVM model on the oversampled training data\n#svc_pipeline =  make_pipeline(StandardScaler(), SVC(random_state = 42))\n#svc_pipeline.fit(X_train_oversampled, y_train_oversampled.values.ravel())\n\nsvc = SVC(random_state = 42)\nsvc.fit(X_train_oversampled, y_train_oversampled.values.ravel())\ny_predicted_svc = svc.predict(X_test_data_scaled)\n\nconfusion_matrix_svc = confusion_matrix(y_test_data, y_predicted_svc)\nprint('Confusion Matrix: ')\nprint(confusion_matrix_svc)\n\nprint('Accuracy (in %): ' + str(svc.score(X_test_data_scaled, y_test_data)))\n\nprecision_svc = precision_score(y_test_data, y_predicted_svc)\nprint('Precision: ' + str(precision_svc))\n\nrecall_svc = recall_score(y_test_data, y_predicted_svc)\nprint('Recall: ' + str(recall_svc))\n\nf1_svc = f1_score(y_test_data, y_predicted_svc)\nprint('F1 score: ' + str(f1_svc))\n\n# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html\nplt.figure(figsize = (10, 7))\nseaborn.heatmap(confusion_matrix_svc, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 3, cbar = True, annot_kws = {'fontsize': 17},\n                xticklabels = ['No stroke (predicted)', 'Stroke (predicted)'], yticklabels = ['No stroke', 'Stroke'])\nplt.yticks(rotation = 0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T17:13:35.228529Z","iopub.execute_input":"2021-11-21T17:13:35.228797Z","iopub.status.idle":"2021-11-21T17:13:37.252154Z","shell.execute_reply.started":"2021-11-21T17:13:35.228769Z","shell.execute_reply":"2021-11-21T17:13:37.251534Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Training and testing a Logistic Regression model on the undersampled training data\nlogistic_reg_undersampled = LogisticRegression(random_state = 42)\nlogistic_reg_undersampled.fit(X_train_undersampled, y_train_undersampled.values.ravel())\n# y_predicted_lr = logistic_reg_pipeline.predict(X_test_data_scaled)\ny_predicted_lr_us = logistic_reg_undersampled.predict(X_test_data_scaled)\n\nconfusion_matrix_lr_us = confusion_matrix(y_test_data, y_predicted_lr_us)\nprint('Confusion Matrix: ')\nprint(confusion_matrix_lr_us)\n\nprint('Accuracy (in %): ' + str(logistic_reg_undersampled.score(X_test_data_scaled, y_test_data)*100))\n\nprecision_lr_us = precision_score(y_test_data, y_predicted_lr_us)\nprint('Precision: ' + str(precision_lr_us))\n\nrecall_lr_us = recall_score(y_test_data, y_predicted_lr_us)\nprint('Recall: ' + str(recall_lr_us))\n\nf1_lr_us = f1_score(y_test_data, y_predicted_lr_us)\nprint('F1 score: ' + str(f1_lr_us))\n\n# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html\nplt.figure(figsize = (10, 7))\nseaborn.heatmap(confusion_matrix_lr_us, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 3, cbar = True, annot_kws = {'fontsize': 17},\n                xticklabels = ['No stroke (predicted)', 'Stroke (predicted)'], yticklabels = ['No stroke', 'Stroke'])\nplt.yticks(rotation = 0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T17:13:49.080828Z","iopub.execute_input":"2021-11-21T17:13:49.081479Z","iopub.status.idle":"2021-11-21T17:13:49.408393Z","shell.execute_reply.started":"2021-11-21T17:13:49.081429Z","shell.execute_reply":"2021-11-21T17:13:49.407543Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Training and testing a SVM model on the undersampled training data\nsvc_undersampled = SVC(random_state = 42)\nsvc_undersampled.fit(X_train_undersampled, y_train_undersampled.values.ravel())\ny_predicted_svc_us = svc_undersampled.predict(X_test_data_scaled)\n\nconfusion_matrix_svc_us = confusion_matrix(y_test_data, y_predicted_svc_us)\nprint('Confusion Matrix: ')\nprint(confusion_matrix_svc_us)\n\nprint('Accuracy (in %): ' + str(svc_undersampled.score(X_test_data_scaled, y_test_data)))\n\nprecision_svc_us = precision_score(y_test_data, y_predicted_svc_us)\nprint('Precision: ' + str(precision_svc_us))\n\nrecall_svc_us = recall_score(y_test_data, y_predicted_svc_us)\nprint('Recall: ' + str(recall_svc_us))\n\nf1_svc_us = f1_score(y_test_data, y_predicted_svc_us)\nprint('F1 score: ' + str(f1_svc_us))\n\n# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html\nplt.figure(figsize = (10, 7))\nseaborn.heatmap(confusion_matrix_svc_us, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 3, cbar = True, annot_kws = {'fontsize': 17},\n                xticklabels = ['No stroke (predicted)', 'Stroke (predicted)'], yticklabels = ['No stroke', 'Stroke'])\nplt.yticks(rotation = 0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T17:13:52.022719Z","iopub.execute_input":"2021-11-21T17:13:52.023145Z","iopub.status.idle":"2021-11-21T17:13:52.352731Z","shell.execute_reply.started":"2021-11-21T17:13:52.023099Z","shell.execute_reply":"2021-11-21T17:13:52.351798Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Training and testing neural network on the oversampled training data\nnn_model = keras.Sequential([\n    keras.layers.Dense(17, input_dim=17, activation='relu'),\n    keras.layers.Dense(15, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nnn_model.compile(optimizer=tf.keras.optimizers.Adam(\n            learning_rate=0.001, beta_1 = 0.9, beta_2=0.99 , epsilon=1e-05,amsgrad=False,name='Adam'\n        ), loss='binary_crossentropy', metrics=['accuracy'])\nnn_model.fit(X_train_oversampled, y_train_oversampled.values.ravel(), epochs=500)\n    \nprint(nn_model.evaluate(X_test_data_scaled, y_test_data))\n    \ny_predicted_nn_model = nn_model.predict(X_test_data_scaled)\ny_predicted_nn_model = numpy.round(y_predicted_nn_model)\n    \nprint(\"Classification Report: \\n\", classification_report(y_test_data, y_predicted_nn_model))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training and testing neural network on the undersampled training data\nnn_model_us = keras.Sequential([\n    keras.layers.Dense(17, input_dim=17, activation='relu'),\n    keras.layers.Dense(15, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nnn_model_us.compile(optimizer=tf.keras.optimizers.Adam(\n            learning_rate=0.001, beta_1 = 0.9, beta_2=0.99 , epsilon=1e-05,amsgrad=False,name='Adam'\n        ), loss='binary_crossentropy', metrics=['accuracy'])\nnn_model_us.fit(X_train_undersampled, y_train_undersampled.values.ravel(), epochs=500)\n    \nprint(nn_model_us.evaluate(X_test_data_scaled, y_test_data))\n    \ny_predicted_nn_model_us = nn_model_us.predict(X_test_data_scaled)\ny_predicted_nn_model_us = numpy.round(y_predicted_nn_model_us)\n    \nprint(\"Classification Report: \\n\", classification_report(y_test_data, y_predicted_nn_model_us))","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","metadata":{"execution":{"iopub.status.busy":"2021-11-21T17:36:16.442527Z","iopub.execute_input":"2021-11-21T17:36:16.443069Z","iopub.status.idle":"2021-11-21T17:36:16.449344Z","shell.execute_reply.started":"2021-11-21T17:36:16.443013Z","shell.execute_reply":"2021-11-21T17:36:16.448120Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Training and testing decision trees on the oversampled training data\ndecision_tree_model = DecisionTreeClassifier(criterion=\"gini\", random_state=42, min_samples_leaf=5, max_depth=3)   \ndecision_tree_model.fit(X_train_oversampled,y_train_oversampled.values.ravel())\ny_predicted_dt = decision_tree_model.predict(X_test_data_scaled)\n# accuracy_score(y_test,y_predicted_dt)\n\nconfusion_matrix_dt = confusion_matrix(y_test_data, y_predicted_dt)\nprint('Confusion Matrix: ')\nprint(confusion_matrix_dt)\n\nprint('Accuracy (in %): ' + str(decision_tree_model.score(X_test_data_scaled, y_test_data)))\n\nprecision_dt = precision_score(y_test_data, y_predicted_dt)\nprint('Precision: ' + str(precision_dt))\n\nrecall_dt = recall_score(y_test_data, y_predicted_dt)\nprint('Recall: ' + str(recall_dt))\n\nf1_dt = f1_score(y_test_data, y_predicted_dt)\nprint('F1 score: ' + str(f1_dt))\n\n# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html\nplt.figure(figsize = (10, 7))\nseaborn.heatmap(confusion_matrix_dt, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 3, cbar = True, annot_kws = {'fontsize': 17},\n                xticklabels = ['No stroke (predicted)', 'Stroke (predicted)'], yticklabels = ['No stroke', 'Stroke'])\nplt.yticks(rotation = 0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T17:42:16.739667Z","iopub.execute_input":"2021-11-21T17:42:16.740739Z","iopub.status.idle":"2021-11-21T17:42:17.044987Z","shell.execute_reply.started":"2021-11-21T17:42:16.740685Z","shell.execute_reply":"2021-11-21T17:42:17.044186Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Training and testing decision trees on the undersampled training data\ndecision_tree_model_us = DecisionTreeClassifier(criterion=\"gini\", random_state=42, min_samples_leaf=5, max_depth=3)  \ndecision_tree_model_us.fit(X_train_undersampled,y_train_undersampled.values.ravel())\ny_predicted_dt_us = decision_tree_model_us.predict(X_test_data_scaled)\n# accuracy_score(y_test,y_predicted_dt)\n\nconfusion_matrix_dt_us = confusion_matrix(y_test_data, y_predicted_dt_us)\nprint('Confusion Matrix: ')\nprint(confusion_matrix_dt_us)\n\nprint('Accuracy (in %): ' + str(decision_tree_model_us.score(X_test_data_scaled, y_test_data)))\n\nprecision_dt_us = precision_score(y_test_data, y_predicted_dt_us)\nprint('Precision: ' + str(precision_dt_us))\n\nrecall_dt_us = recall_score(y_test_data, y_predicted_dt_us)\nprint('Recall: ' + str(recall_dt_us))\n\nf1_dt_us = f1_score(y_test_data, y_predicted_dt_us)\nprint('F1 score: ' + str(f1_dt_us))\n\n# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html\nplt.figure(figsize = (10, 7))\nseaborn.heatmap(confusion_matrix_dt_us, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 3, cbar = True, annot_kws = {'fontsize': 17},\n                xticklabels = ['No stroke (predicted)', 'Stroke (predicted)'], yticklabels = ['No stroke', 'Stroke'])\nplt.yticks(rotation = 0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T17:53:54.241295Z","iopub.execute_input":"2021-11-21T17:53:54.243035Z","iopub.status.idle":"2021-11-21T17:53:54.554909Z","shell.execute_reply.started":"2021-11-21T17:53:54.242933Z","shell.execute_reply":"2021-11-21T17:53:54.554095Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\n# Training and testing adaboost on the oversampled training data\nadaboost_classifier = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n# Train Adaboost Classifer\nadaboost_model = adaboost_classifier.fit(X_train_oversampled, y_train_oversampled.values.ravel())\n\ny_predicted_adaboost = adaboost_model.predict(X_test_data_scaled)\n# accuracy_score(y_test,y_predicted_dt)\n\nconfusion_matrix_adaboost = confusion_matrix(y_test_data, y_predicted_adaboost)\nprint('Confusion Matrix: ')\nprint(confusion_matrix_adaboost)\n\nprint('Accuracy (in %): ' + str(adaboost_model.score(X_test_data_scaled, y_test_data)))\n\nprecision_adaboost = precision_score(y_test_data, y_predicted_adaboost)\nprint('Precision: ' + str(precision_adaboost))\n\nrecall_adaboost = recall_score(y_test_data, y_predicted_adaboost)\nprint('Recall: ' + str(recall_adaboost))\n\nf1_adaboost = f1_score(y_test_data, y_predicted_adaboost)\nprint('F1 score: ' + str(f1_adaboost))\n\n# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html\nplt.figure(figsize = (10, 7))\nseaborn.heatmap(confusion_matrix_adaboost, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 3, cbar = True, annot_kws = {'fontsize': 17},\n                xticklabels = ['No stroke (predicted)', 'Stroke (predicted)'], yticklabels = ['No stroke', 'Stroke'])\nplt.yticks(rotation = 0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T18:00:12.916878Z","iopub.execute_input":"2021-11-21T18:00:12.918660Z","iopub.status.idle":"2021-11-21T18:00:13.598587Z","shell.execute_reply.started":"2021-11-21T18:00:12.918590Z","shell.execute_reply":"2021-11-21T18:00:13.597554Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Training and testing adaboost on the undersampled training data\nadaboost_classifier_us = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n# Train Adaboost Classifer\nadaboost_model_us = adaboost_classifier_us.fit(X_train_undersampled, y_train_undersampled.values.ravel())\n\ny_predicted_adaboost_us = adaboost_model_us.predict(X_test_data_scaled)\n# accuracy_score(y_test,y_predicted_dt)\n\nconfusion_matrix_adaboost_us = confusion_matrix(y_test_data, y_predicted_adaboost_us)\nprint('Confusion Matrix: ')\nprint(confusion_matrix_adaboost_us)\n\nprint('Accuracy (in %): ' + str(adaboost_model_us.score(X_test_data_scaled, y_test_data)))\n\nprecision_adaboost_us = precision_score(y_test_data, y_predicted_adaboost_us)\nprint('Precision: ' + str(precision_adaboost_us))\n\nrecall_adaboost_us = recall_score(y_test_data, y_predicted_adaboost_us)\nprint('Recall: ' + str(recall_adaboost_us))\n\nf1_adaboost_us = f1_score(y_test_data, y_predicted_adaboost_us)\nprint('F1 score: ' + str(f1_adaboost_us))\n\n# Reference: https://seaborn.pydata.org/generated/seaborn.heatmap.html\nplt.figure(figsize = (10, 7))\nseaborn.heatmap(confusion_matrix_adaboost_us, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 3, cbar = True, annot_kws = {'fontsize': 17},\n                xticklabels = ['No stroke (predicted)', 'Stroke (predicted)'], yticklabels = ['No stroke', 'Stroke'])\nplt.yticks(rotation = 0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T18:05:24.903849Z","iopub.execute_input":"2021-11-21T18:05:24.904294Z","iopub.status.idle":"2021-11-21T18:05:25.310579Z","shell.execute_reply.started":"2021-11-21T18:05:24.904243Z","shell.execute_reply":"2021-11-21T18:05:25.309182Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}